{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac3d081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3920d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data and label\n",
    "x1 = [ 73., 93., 89., 96., 73.]\n",
    "x2 = [ 80., 88., 91., 98., 66.]\n",
    "x3 = [ 75., 93., 90.,100., 70.]\n",
    "Y = [ 152., 185., 180., 196., 142.]\n",
    "\n",
    "# weights\n",
    "w1 = tf.Variable(tf.random.normal([1]))\n",
    "w2 = tf.Variable(tf.random.normal([1]))\n",
    "w3 = tf.Variable(tf.random.normal([1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9824a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0 |    9087.4473\n",
      "    50 |     184.1915\n",
      "   100 |      85.1757\n",
      "   150 |      83.8530\n",
      "   200 |      83.6147\n",
      "   250 |      83.3890\n",
      "   300 |      83.1643\n",
      "   350 |      82.9399\n",
      "   400 |      82.7162\n",
      "   450 |      82.4934\n",
      "   500 |      82.2710\n",
      "   550 |      82.0490\n",
      "   600 |      81.8280\n",
      "   650 |      81.6074\n",
      "   700 |      81.3873\n",
      "   750 |      81.1679\n",
      "   800 |      80.9492\n",
      "   850 |      80.7310\n",
      "   900 |      80.5133\n",
      "   950 |      80.2962\n",
      "  1000 |      80.0799\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.000001\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # \n",
    "    with tf.GradientTape() as tape: # 변수들을 tape 에 기록\n",
    "        hypothesis  = w1*x1 + w2*x2 + w3*x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    # tape 에 기록된 w1,2,3,b 를 각각의 gradint 에 할당\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\" {:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1588780",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    [73.,80.,75.,152.],\n",
    "    [93.,88.,93.,185.],\n",
    "    [89.,91.,90.,180.],\n",
    "    [96.,98.,100.,196.],\n",
    "    [73.,66.,70.,142.]\n",
    "], dtype=np.float32)\n",
    "\n",
    "#slice data\n",
    "X = data[:,:-1] # 콜론 기준 앞은 행, 뒤는 열, 모든 row 의 마지막 col 을 제외한다는뜻\n",
    "Y = data[:,[-1]] # 모든 row 의 마지막 col 만 사용한다는 뜻\n",
    "# row = 3, 출력은 1 즉 wieght = 3-1 행렬         \n",
    "W = tf.Variable(tf.random.normal([3,1])) \n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "         \n",
    "def predict(X):\n",
    "    # matmul = matrix multifulication\n",
    "    return tf.matmul(X, W) + b # 나중에는 b 를 생략 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c24af9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0 |       0.6823 |       1.3179\n",
      "   100 |       0.6822 |       1.3164\n",
      "   200 |       0.6821 |       1.3149\n",
      "   300 |       0.6820 |       1.3134\n",
      "   400 |       0.6819 |       1.3118\n",
      "   500 |       0.6819 |       1.3103\n",
      "   600 |       0.6818 |       1.3088\n",
      "   700 |       0.6817 |       1.3073\n",
      "   800 |       0.6816 |       1.3059\n",
      "   900 |       0.6815 |       1.3044\n",
      "  1000 |       0.6814 |       1.3029\n",
      "  1100 |       0.6814 |       1.3014\n",
      "  1200 |       0.6813 |       1.3000\n",
      "  1300 |       0.6812 |       1.2985\n",
      "  1400 |       0.6811 |       1.2971\n",
      "  1500 |       0.6810 |       1.2956\n",
      "  1600 |       0.6809 |       1.2942\n",
      "  1700 |       0.6809 |       1.2927\n",
      "  1800 |       0.6808 |       1.2913\n",
      "  1900 |       0.6807 |       1.2899\n",
      "  2000 |       0.6806 |       1.2885\n",
      "  2100 |       0.6805 |       1.2870\n",
      "  2200 |       0.6805 |       1.2856\n",
      "  2300 |       0.6804 |       1.2842\n",
      "  2400 |       0.6803 |       1.2828\n",
      "  2500 |       0.6802 |       1.2814\n",
      "  2600 |       0.6801 |       1.2801\n",
      "  2700 |       0.6801 |       1.2787\n",
      "  2800 |       0.6800 |       1.2773\n",
      "  2900 |       0.6799 |       1.2759\n",
      "  3000 |       0.6798 |       1.2745\n",
      "  3100 |       0.6798 |       1.2732\n",
      "  3200 |       0.6797 |       1.2718\n",
      "  3300 |       0.6796 |       1.2705\n",
      "  3400 |       0.6796 |       1.2691\n",
      "  3500 |       0.6795 |       1.2678\n",
      "  3600 |       0.6794 |       1.2664\n",
      "  3700 |       0.6793 |       1.2651\n",
      "  3800 |       0.6793 |       1.2637\n",
      "  3900 |       0.6792 |       1.2624\n",
      "  4000 |       0.6791 |       1.2611\n",
      "  4100 |       0.6791 |       1.2598\n",
      "  4200 |       0.6790 |       1.2585\n",
      "  4300 |       0.6789 |       1.2571\n",
      "  4400 |       0.6789 |       1.2559\n",
      "  4500 |       0.6788 |       1.2545\n",
      "  4600 |       0.6787 |       1.2532\n",
      "  4700 |       0.6787 |       1.2519\n",
      "  4800 |       0.6786 |       1.2506\n",
      "  4900 |       0.6785 |       1.2494\n",
      "  5000 |       0.6785 |       1.2481\n",
      "  5100 |       0.6784 |       1.2468\n",
      "  5200 |       0.6784 |       1.2455\n",
      "  5300 |       0.6783 |       1.2442\n",
      "  5400 |       0.6782 |       1.2430\n",
      "  5500 |       0.6782 |       1.2417\n",
      "  5600 |       0.6781 |       1.2405\n",
      "  5700 |       0.6781 |       1.2392\n",
      "  5800 |       0.6780 |       1.2380\n",
      "  5900 |       0.6779 |       1.2367\n",
      "  6000 |       0.6779 |       1.2355\n",
      "  6100 |       0.6778 |       1.2343\n",
      "  6200 |       0.6778 |       1.2330\n",
      "  6300 |       0.6777 |       1.2318\n",
      "  6400 |       0.6776 |       1.2306\n",
      "  6500 |       0.6776 |       1.2294\n",
      "  6600 |       0.6775 |       1.2282\n",
      "  6700 |       0.6775 |       1.2270\n",
      "  6800 |       0.6774 |       1.2258\n",
      "  6900 |       0.6774 |       1.2246\n",
      "  7000 |       0.6773 |       1.2234\n",
      "  7100 |       0.6773 |       1.2222\n",
      "  7200 |       0.6772 |       1.2210\n",
      "  7300 |       0.6772 |       1.2198\n",
      "  7400 |       0.6771 |       1.2187\n",
      "  7500 |       0.6771 |       1.2175\n",
      "  7600 |       0.6770 |       1.2164\n",
      "  7700 |       0.6769 |       1.2152\n",
      "  7800 |       0.6769 |       1.2141\n",
      "  7900 |       0.6768 |       1.2129\n",
      "  8000 |       0.6768 |       1.2118\n",
      "  8100 |       0.6767 |       1.2107\n",
      "  8200 |       0.6767 |       1.2095\n",
      "  8300 |       0.6766 |       1.2084\n",
      "  8400 |       0.6766 |       1.2073\n",
      "  8500 |       0.6765 |       1.2062\n",
      "  8600 |       0.6765 |       1.2051\n",
      "  8700 |       0.6764 |       1.2040\n",
      "  8800 |       0.6764 |       1.2029\n",
      "  8900 |       0.6763 |       1.2018\n",
      "  9000 |       0.6763 |       1.2007\n",
      "  9100 |       0.6762 |       1.1996\n",
      "  9200 |       0.6762 |       1.1985\n",
      "  9300 |       0.6761 |       1.1974\n",
      "  9400 |       0.6761 |       1.1963\n",
      "  9500 |       0.6760 |       1.1953\n",
      "  9600 |       0.6760 |       1.1942\n",
      "  9700 |       0.6760 |       1.1931\n",
      "  9800 |       0.6759 |       1.1920\n",
      "  9900 |       0.6759 |       1.1910\n",
      " 10000 |       0.6758 |       1.1899\n",
      " 10100 |       0.6758 |       1.1889\n",
      " 10200 |       0.6757 |       1.1878\n",
      " 10300 |       0.6757 |       1.1868\n",
      " 10400 |       0.6757 |       1.1857\n",
      " 10500 |       0.6756 |       1.1847\n",
      " 10600 |       0.6756 |       1.1836\n",
      " 10700 |       0.6755 |       1.1826\n",
      " 10800 |       0.6755 |       1.1816\n",
      " 10900 |       0.6754 |       1.1805\n",
      " 11000 |       0.6754 |       1.1795\n",
      " 11100 |       0.6754 |       1.1785\n",
      " 11200 |       0.6753 |       1.1774\n",
      " 11300 |       0.6753 |       1.1764\n",
      " 11400 |       0.6753 |       1.1754\n",
      " 11500 |       0.6752 |       1.1744\n",
      " 11600 |       0.6752 |       1.1734\n",
      " 11700 |       0.6751 |       1.1724\n",
      " 11800 |       0.6751 |       1.1714\n",
      " 11900 |       0.6751 |       1.1704\n",
      " 12000 |       0.6750 |       1.1694\n",
      " 12100 |       0.6750 |       1.1684\n",
      " 12200 |       0.6750 |       1.1674\n",
      " 12300 |       0.6749 |       1.1664\n",
      " 12400 |       0.6749 |       1.1654\n",
      " 12500 |       0.6749 |       1.1644\n",
      " 12600 |       0.6748 |       1.1634\n",
      " 12700 |       0.6748 |       1.1624\n",
      " 12800 |       0.6748 |       1.1615\n",
      " 12900 |       0.6747 |       1.1605\n",
      " 13000 |       0.6747 |       1.1595\n",
      " 13100 |       0.6747 |       1.1586\n",
      " 13200 |       0.6747 |       1.1576\n",
      " 13300 |       0.6746 |       1.1566\n",
      " 13400 |       0.6746 |       1.1557\n",
      " 13500 |       0.6746 |       1.1547\n",
      " 13600 |       0.6745 |       1.1537\n",
      " 13700 |       0.6745 |       1.1528\n",
      " 13800 |       0.6745 |       1.1518\n",
      " 13900 |       0.6744 |       1.1509\n",
      " 14000 |       0.6744 |       1.1499\n",
      " 14100 |       0.6744 |       1.1490\n",
      " 14200 |       0.6744 |       1.1480\n",
      " 14300 |       0.6744 |       1.1471\n",
      " 14400 |       0.6743 |       1.1462\n",
      " 14500 |       0.6743 |       1.1453\n",
      " 14600 |       0.6743 |       1.1443\n",
      " 14700 |       0.6743 |       1.1434\n",
      " 14800 |       0.6742 |       1.1425\n",
      " 14900 |       0.6742 |       1.1416\n",
      " 15000 |       0.6742 |       1.1407\n",
      " 15100 |       0.6742 |       1.1398\n",
      " 15200 |       0.6741 |       1.1389\n",
      " 15300 |       0.6741 |       1.1379\n",
      " 15400 |       0.6741 |       1.1371\n",
      " 15500 |       0.6741 |       1.1362\n",
      " 15600 |       0.6740 |       1.1353\n",
      " 15700 |       0.6740 |       1.1344\n",
      " 15800 |       0.6740 |       1.1335\n",
      " 15900 |       0.6740 |       1.1326\n",
      " 16000 |       0.6739 |       1.1318\n",
      " 16100 |       0.6739 |       1.1309\n",
      " 16200 |       0.6739 |       1.1300\n",
      " 16300 |       0.6739 |       1.1292\n",
      " 16400 |       0.6738 |       1.1283\n",
      " 16500 |       0.6738 |       1.1275\n",
      " 16600 |       0.6738 |       1.1266\n",
      " 16700 |       0.6738 |       1.1257\n",
      " 16800 |       0.6738 |       1.1249\n",
      " 16900 |       0.6738 |       1.1240\n",
      " 17000 |       0.6737 |       1.1232\n",
      " 17100 |       0.6737 |       1.1223\n",
      " 17200 |       0.6737 |       1.1215\n",
      " 17300 |       0.6737 |       1.1207\n",
      " 17400 |       0.6737 |       1.1198\n",
      " 17500 |       0.6736 |       1.1190\n",
      " 17600 |       0.6736 |       1.1182\n",
      " 17700 |       0.6736 |       1.1174\n",
      " 17800 |       0.6736 |       1.1165\n",
      " 17900 |       0.6736 |       1.1157\n",
      " 18000 |       0.6736 |       1.1149\n",
      " 18100 |       0.6735 |       1.1141\n",
      " 18200 |       0.6735 |       1.1132\n",
      " 18300 |       0.6735 |       1.1124\n",
      " 18400 |       0.6735 |       1.1116\n",
      " 18500 |       0.6735 |       1.1108\n",
      " 18600 |       0.6735 |       1.1100\n",
      " 18700 |       0.6735 |       1.1092\n",
      " 18800 |       0.6734 |       1.1084\n",
      " 18900 |       0.6734 |       1.1076\n",
      " 19000 |       0.6734 |       1.1068\n",
      " 19100 |       0.6734 |       1.1060\n",
      " 19200 |       0.6734 |       1.1052\n",
      " 19300 |       0.6734 |       1.1044\n",
      " 19400 |       0.6734 |       1.1036\n",
      " 19500 |       0.6734 |       1.1028\n",
      " 19600 |       0.6733 |       1.1021\n",
      " 19700 |       0.6733 |       1.1013\n",
      " 19800 |       0.6733 |       1.1005\n",
      " 19900 |       0.6733 |       1.0997\n",
      " 20000 |       0.6733 |       1.0989\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.000001\n",
    "\n",
    "n_epochs = 20000\n",
    "for i in range(n_epochs+1):\n",
    "     with tf.GradientTape() as tape: # 변수들을 tape 에 기록\n",
    "        cost = tf.reduce_mean(tf.square(predict(X) - Y))\n",
    "        \n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        \n",
    "        # 업데이트\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "        if i % 100 == 0 :\n",
    "           # 차수 , 0번째의 가중치, 코스트 \n",
    "            print(\" {:5} | {:12.4f} | {:12.4f}\".format(i,W.numpy()[0][0], cost.numpy()))\n",
    "        \n",
    "        # 메트릭스를 써주는게 성능면에서 굉장히 유리하다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e77000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
